{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a27e54-c74b-4039-962e-99a927b9c0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL.Image import Image as PilImage\n",
    "import textwrap, os\n",
    "import sagemaker\n",
    "from sagemaker.serverless.serverless_inference_config import ServerlessInferenceConfig\n",
    "from sagemaker.serializers import JSONSerializer, IdentitySerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301147a-caf2-4728-87cf-eb87d8ed6daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_bucket_name = sagemaker.session.Session().default_bucket()\n",
    "s3_bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c07c03-c302-4fe7-a8d1-8193a5a44b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile build_model_tar.sh\n",
    "#!/bin/bash\n",
    "BUCKET_NAME=\"$1\"\n",
    "MODEL_NAME=RN50.pt\n",
    "MODEL_NAME_URL=https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\n",
    "\n",
    "BUILD_ROOT=/tmp/model_path\n",
    "S3_PATH=s3://${BUCKET_NAME}/models/clip/model.tar.gz\n",
    "\n",
    "\n",
    "rm -rf $BUILD_ROOT\n",
    "mkdir $BUILD_ROOT\n",
    "cd $BUILD_ROOT && curl -o $BUILD_ROOT/$MODEL_NAME $MODEL_NAME_URL\n",
    "cd $BUILD_ROOT && tar -czvf model.tar.gz .\n",
    "aws s3 cp $BUILD_ROOT/model.tar.gz  $S3_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8034ce60-a2d0-43e8-ae12-c594f0a3db25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!bash build_model_tar.sh {s3_bucket_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8a29a-a051-4f63-8ebf-758b64d70770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls s3://{s3_bucket_name}/models/clip/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304fc0d-a333-4ef7-b2e7-2bfae7bb0c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a06d48a-1758-401b-ab9d-e92627dd2ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code/clip_inference.py\n",
    "\n",
    "import io\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"RN50.pt\")\n",
    "# ENCODE_TYPE could be IMAGE or TEXT\n",
    "ENCODE_TYPE = os.environ.get(\"ENCODE_TYPE\", \"TEXT\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# defining model and loading weights to it.\n",
    "def model_fn(model_dir):\n",
    "    model, preprocess = clip.load(os.path.join(model_dir, MODEL_NAME), device=device)\n",
    "    return {\"model_obj\": model, \"preprocess_fn\": preprocess}\n",
    "\n",
    "\n",
    "def load_from_bytearray(request_body):\n",
    "    \n",
    "    return image\n",
    "\n",
    "# data loading\n",
    "def input_fn(request_body, request_content_type):\n",
    "    assert request_content_type in (\n",
    "        \"application/json\",\n",
    "        \"application/x-image\",\n",
    "    ), f\"{request_content_type} is an unknown type.\"\n",
    "    if request_content_type == \"application/json\":\n",
    "        data = json.loads(request_body)[\"inputs\"]\n",
    "    elif request_content_type == \"application/x-image\":\n",
    "        image_as_bytes = io.BytesIO(request_body)\n",
    "        data = Image.open(image_as_bytes)\n",
    "    return data\n",
    "\n",
    "\n",
    "# inference\n",
    "def predict_fn(input_object, model):\n",
    "    model_obj = model[\"model_obj\"]\n",
    "    # for image preprocessing\n",
    "    preprocess_fn = model[\"preprocess_fn\"]\n",
    "    assert ENCODE_TYPE in (\"TEXT\", \"IMAGE\"), f\"{ENCODE_TYPE} is an unknown encode type.\"\n",
    "\n",
    "    # preprocessing\n",
    "    if ENCODE_TYPE == \"TEXT\":\n",
    "        input_ = clip.tokenize(input_object).to(device)\n",
    "    elif ENCODE_TYPE == \"IMAGE\":\n",
    "        input_ = preprocess_fn(input_object).unsqueeze(0).to(device)\n",
    "\n",
    "    # inference\n",
    "    with torch.no_grad():\n",
    "        if ENCODE_TYPE == \"TEXT\":\n",
    "            prediction = model_obj.encode_text(input_)\n",
    "        elif ENCODE_TYPE == \"IMAGE\":\n",
    "            prediction = model_obj.encode_image(input_)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# Serialize the prediction result into the desired response content type\n",
    "def output_fn(predictions, content_type):\n",
    "    assert content_type == \"application/json\"\n",
    "    res = predictions.cpu().numpy().tolist()\n",
    "    return json.dumps(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97066e4e-c198-4317-848f-52cb4f9b32ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "ftfy\n",
    "regex\n",
    "tqdm\n",
    "git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a5f5ba-3d2e-4daf-bb64-e15c85164438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role, Session\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "shared_params = dict(\n",
    "    entry_point=\"clip_inference.py\",\n",
    "    source_dir=\"code\",\n",
    "    role=role,\n",
    "    model_data=f\"s3://{s3_bucket_name}/models/clip/model.tar.gz\",\n",
    "    framework_version=\"1.9.0\",\n",
    "    py_version=\"py38\",\n",
    ")\n",
    "\n",
    "clip_image_model = PyTorchModel(\n",
    "    env={'MODEL_NAME': 'RN50.pt', \"ENCODE_TYPE\": \"IMAGE\"},\n",
    "    name=\"clip-image-model\",\n",
    "    **shared_params\n",
    ")\n",
    "\n",
    "clip_text_model = PyTorchModel(\n",
    "    env={'MODEL_NAME': 'RN50.pt', \"ENCODE_TYPE\": \"TEXT\"},\n",
    "    name=\"clip-text-model\",\n",
    "    **shared_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07533312-bd18-408e-b1f9-fcb379ea6fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_predictor = clip_text_model.deploy(\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    initial_instance_count=1,\n",
    "    serverless_inference_config=ServerlessInferenceConfig(memory_size_in_mb=6144),\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "image_predictor = clip_image_model.deploy(\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    initial_instance_count=1,\n",
    "    serverless_inference_config=ServerlessInferenceConfig(memory_size_in_mb=6144),\n",
    "    serializer=IdentitySerializer(content_type=\"application/x-image\"),\n",
    "    deserializer=JSONDeserializer(),\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "def encode_image(file_name):    \n",
    "    with open(file_name, \"rb\") as f:\n",
    "        payload = f.read()\n",
    "        payload = bytearray(payload)\n",
    "    res = image_predictor.predict(payload)\n",
    "    return res[0]\n",
    "\n",
    "def encode_name(item_name):\n",
    "    res = text_predictor.predict({\"inputs\": [f\"this is a {item_name}\"]})\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5d717-9e38-4bb3-9f05-b4f9e935bd02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = 'fishes'\n",
    "text_emb = encode_name(text)\n",
    "print(len(text_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16b197-84b6-4120-ad97-f43c02208f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = 'test-image-clip.jpeg'\n",
    "image_emb = encode_image(image)\n",
    "print(len(image_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69edbac2-2f6e-46dc-b727-a28181faec5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "text_emb = np.array(text_emb)\n",
    "image_emb = np.array(image_emb)\n",
    "cos_sim = text_emb.dot(image_emb) / (np.linalg.norm(text_emb) * np.linalg.norm(image_emb))\n",
    "print(cos_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
